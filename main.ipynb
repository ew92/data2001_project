{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA2001 S1 2021 Practical Assignment - Bushfire Risk Analysis (Notebook)\n",
    "*An analysis of neighbourhood fire risk and median income & rent.*\n",
    "\n",
    "**Assignment Group F14 - 3**\n",
    "\n",
    "**Eugene Ward (SID: 311193781) & Matthew Shu (SID: 500445930)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import libraries and create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "# Function for accessing Postgres DB (SOURCE: DATA2001 Lab materials) - Eugene uses this\n",
    "def pgconnect_using_credfile(credential_filepath):\n",
    "    try:\n",
    "        with open(credential_filepath) as f:\n",
    "            db_conn_dict = json.load(f)\n",
    "        connstring = 'postgres+psycopg2://'+db_conn_dict['user']+':'+db_conn_dict['password']+'@'+db_conn_dict['host']+'/'+db_conn_dict['database']\n",
    "        db = create_engine(connstring, echo=False)\n",
    "        conn = db.connect()\n",
    "        print('connected')\n",
    "    except Exception as e:\n",
    "        print(\"unable to connect to the database\")\n",
    "        print(e)\n",
    "        return None\n",
    "    return db,conn\n",
    "\n",
    "# Function for querying the PostgreSQL DB.\n",
    "# Returns value and a converted dataframe (SOURCE: DATA2001 Lab materials)\n",
    "def pgquery( conn, sqlcmd, args=None, silent=False ):\n",
    "    \"\"\" utility function to execute some SQL query statement\n",
    "    can take optional arguments to fill in (dictionary)\n",
    "    will print out on screen the result set of the query\n",
    "    error and transaction handling built-in \"\"\"\n",
    "    retdf = pd.DataFrame()\n",
    "    retval = False\n",
    "    try:\n",
    "        if args is None:\n",
    "            retdf = pd.read_sql_query(sqlcmd,conn)\n",
    "        else:\n",
    "            retdf = pd.read_sql_query(sqlcmd,conn,params=args)\n",
    "        if silent == False:\n",
    "            print(retdf.shape)\n",
    "            print(retdf.to_string())\n",
    "        retval = True\n",
    "    except Exception as e:\n",
    "        if silent == False:\n",
    "            print(\"db read error: \")\n",
    "            print(e)\n",
    "    return retval,retdf\n",
    "\n",
    "# WKT point geom creation function (SOURCE: DATA2001 Lab materials)\n",
    "def create_wkt_point_element(geom,srid):\n",
    "    return WKTElement(geom.wkt, srid)\n",
    "\n",
    "# WKT polygon geom creation function (SOURCE: DATA2001 Lab materials)\n",
    "# Adapted to handle conversion of empty geometries\n",
    "def create_wkt_element(geom,srid):\n",
    "    if (geom.geom_type == 'Polygon'):\n",
    "        geom = MultiPolygon([geom])\n",
    "    elif (geom.geom_type == 'GeometryCollection'):\n",
    "        geom = MultiPolygon([geom])\n",
    "    return WKTElement(geom.wkt, srid)\n",
    "\n",
    "# Z-score\n",
    "def z(x, avg, sd):\n",
    "    return((x-avg)/sd)\n",
    "\n",
    "# Sigmoidal function. Did not use native exponential because fails for large negative values.\n",
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "# Created and add z-score column for measure\n",
    "def add_z_score_column(df, **kwargs):\n",
    "    column = [c for c in df.columns.tolist() if 'density' in c][0] #Finds relevant column to calculate Z-score on\n",
    "    if kwargs.get('column'):\n",
    "        column = kwargs.get('column')\n",
    "    mean = np.mean(df[column])\n",
    "    std = np.std(df[column])\n",
    "    df['z_score'] = df[column].map(lambda x: z(x, mean, std))\n",
    "    return df\n",
    "\n",
    "# Generate correlation matrix\n",
    "def CorrHeatmap(df, X):\n",
    "    cor = df[['fire_risk_score', X]].corr() #Native pandas correlation\n",
    "    sns.heatmap(cor, annot=True)\n",
    "    plt.title('Correlation')\n",
    "    plt.show()\n",
    "\n",
    "# Generate scatter chart with LM line\n",
    "def LinearRegGraph(reg, y_array, X_array, X):\n",
    "    plt.scatter(X_array, y_array)\n",
    "    y_pred = reg.predict(X_array)\n",
    "    plt.plot(X_array, y_pred)\n",
    "    plt.xlabel('{X}'.format(X=X))\n",
    "    plt.ylabel('Fire Risk Score')\n",
    "    plt.title('Fire Risk Score regressed on {X}'.format(X=X))\n",
    "    plt.show()\n",
    "\n",
    "# Generate residuals plot\n",
    "def Residual(reg, y_array, X_array, X): #Condition for creating a residual plot\n",
    "    y_pred = reg.predict(X_array)\n",
    "    res = y_array-y_pred\n",
    "    plt.scatter(y_pred, res)\n",
    "    plt.xlabel('Predicted values for {X}'.format(X=X))\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals for {X}'.format(X=X))\n",
    "    plt.axhline(color='r')\n",
    "    plt.show()\n",
    "\n",
    "# Generate OLS details and test values\n",
    "def Stats(df, y_array, X_array):\n",
    "    X2 = sm.add_constant(X_array)\n",
    "    mod = sm.OLS(y_array, X2)\n",
    "    res = mod.fit()\n",
    "    print(res.summary())\n",
    "    \n",
    "# Generate all correlation analyses for values returned from our database, using functions defined above\n",
    "def LinearReg(y, X):\n",
    "    query = \"\"\"SELECT area_id, fire_risk_score, {X} FROM neighbourhoods INNER JOIN fire_risk USING(area_id);\"\"\".format(X=X)\n",
    "    response, df = pgquery(conn, query)\n",
    "    df = df.dropna() #Drop rows with NaN median income and monthly rent\n",
    "    print(df)\n",
    "    y_array = np.array(df.loc[:,y])\n",
    "    X_array = np.array(df.loc[:, X]).reshape(-1,1) #Sklearn requires multidimensional\n",
    "    reg = LinearRegression().fit(X_array,y_array)\n",
    "    Residual(reg, y_array, X_array, X)\n",
    "    LinearRegGraph(reg, y_array, X_array, X)\n",
    "    Stats(df, y_array, X_array)\n",
    "    CorrHeatmap(df,X)\n",
    "    return reg\n",
    "\n",
    "print(\"Setup successful. Libraries imported and functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Internal use MATTHEW\n",
    "#For troubleshooting in the case your data isn't loading.\n",
    "#os.chdir('M:\\\\Jupyter Notebooks\\\\data2001_project')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Integration and Database Generation\n",
    "\n",
    "The 3 provided CSV files and 2 provided shapefiles are loaded in as dataframes/geodataframes, inspected and cleaned. We are also making use of one additional dataset, retrieved from a Geoscience Australia Web Service and converted to a geodataframe.\n",
    "\n",
    "### Loading datasets (standard dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Statistical Areas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_areas_df = pd.read_csv('./data/StatisticalAreas.csv')\n",
    "print(stat_areas_df.dtypes)\n",
    "\n",
    "# Cleaning - remove duplicates\n",
    "print(len(stat_areas_df.area_id.unique()))\n",
    "\n",
    "# DUPLICATE ROWS in statisticalareas - confirmed\n",
    "#print(stat_areas_df.area_id.value_counts())\n",
    "#stat_areas_df.loc[stat_areas_df['area_id']==106]\n",
    "#stat_areas_df.loc[stat_areas_df['area_id']==111]\n",
    "#stat_areas_df.loc[stat_areas_df['area_id']==114]\n",
    "\n",
    "print(len(stat_areas_df))\n",
    "stat_areas_df = stat_areas_df.drop_duplicates() # DECISION: Remove duplicate rows\n",
    "print(len(stat_areas_df))\n",
    "\n",
    "stat_areas_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Neighbourhoods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbhd_df = pd.read_csv('./data/Neighbourhoods.csv')\n",
    "\n",
    "#nbhd_df.area_id.value_counts() # confirmed no duplicate area_id\n",
    "\n",
    "# Changing column names according to assignment sheet\n",
    "nbhd_df = nbhd_df.rename(columns={'number_of_dwellings':'dwellings', 'number_of_businesses':'businesses', 'median_annual_household_income':'median_income'}) \n",
    "print(len(nbhd_df))\n",
    "print(nbhd_df.dtypes)\n",
    "\n",
    "# Cleaning - correcting number representations\n",
    "nbhd_df['population'] = nbhd_df['population'].str.replace(',','')\n",
    "nbhd_df['population'] = nbhd_df['population'].astype('float64')\n",
    "nbhd_df['dwellings'] = nbhd_df['dwellings'].str.replace(',','')\n",
    "nbhd_df['dwellings'] = nbhd_df['dwellings'].astype('float64')\n",
    "\n",
    "# Note: NaN values present in neighbourhoods\n",
    "# Decision for later work - interpet NaN as 0\n",
    "print(len(nbhd_df[nbhd_df.isna().any(axis=1)]))\n",
    "nbhd_df[nbhd_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Business Stats*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busi_stat_df = pd.read_csv('./data/BusinessStats.csv')\n",
    "\n",
    "busi_stat_df = busi_stat_df.drop(columns=['area_name']) # Not needed in our DB design\n",
    "busi_stat_df = busi_stat_df.rename(columns={'accommodation_and_food_services':'accommodation_and_food'}) # Changing column naming according to assignment sheet\n",
    "\n",
    "#busi_stat_df.area_id.value_counts() # confirmed no duplicates\n",
    "print(len(busi_stat_df))\n",
    "print(busi_stat_df.dtypes)\n",
    "\n",
    "# DB DESIGN DECISION: LIMIT BUSINESS STAT OBSERVATIONS IN DB TO THE NEIGHBOURHOODS STUDIED\n",
    "\n",
    "print(len(busi_stat_df))\n",
    "busi_stat_df = busi_stat_df.loc[busi_stat_df['area_id'].isin(nbhd_df['area_id'].tolist())]\n",
    "print(len(busi_stat_df))\n",
    "\n",
    "# Note: No NAN values present in businessstats\n",
    "#busi_stat_df[busi_stat_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION: cast columns as req. in each df as string, as integrity check before going into DB as non-numeric values\n",
    "stat_areas_df['area_id'] = stat_areas_df['area_id'].astype('str')\n",
    "print(stat_areas_df['area_id'].dtype)\n",
    "\n",
    "nbhd_df['area_id'] = nbhd_df['area_id'].astype('str')\n",
    "print(nbhd_df['area_id'].dtype)\n",
    "\n",
    "busi_stat_df['area_id'] = busi_stat_df['area_id'].astype('str')\n",
    "print(busi_stat_df['area_id'].dtype)\n",
    "\n",
    "stat_areas_df['parent_area_id'] = stat_areas_df['parent_area_id'].astype('str')\n",
    "print(stat_areas_df['parent_area_id'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets (geodataframes)\n",
    "#### RFS NSW Bushfire Prone Land - shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs_gdf = gpd.read_file('./data/RFSNSW_BFPL/RFSNSW_BFPL.shp')\n",
    "print(rfs_gdf.crs) # Check EPSG / CRS -- 4283 = GDA94\n",
    "rfs_gdf.columns = [x.lower() for x in rfs_gdf.columns] # lower case col names\n",
    "print(rfs_gdf.dtypes)\n",
    "\n",
    "# Check geometries\n",
    "print(len(rfs_gdf))\n",
    "rfs_gdf.geometry.type.value_counts()\n",
    "\n",
    "# Recreate incrementing 'gid' (0 index)\n",
    "rfs_gdf.insert(loc=0, column='gid', value=rfs_gdf.index)\n",
    "rfs_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ABS Statistical Area 2 (2016) - shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_gdf = gpd.read_file('./data/1270055001_sa2_2016_aust_shape/SA2_2016_AUST.shp')\n",
    "\n",
    "print(sa2_gdf.crs) # Check EPSG / CRS -- 4283 = GDA94\n",
    "sa2_gdf.columns = [x.lower() for x in sa2_gdf.columns] # lower case col names\n",
    "\n",
    "# Recreate incrementing 'g_id' (0 index)\n",
    "sa2_gdf.insert(loc=0, column='g_id', value=sa2_gdf.index)\n",
    "\n",
    "# Changing column names according to assignment sheet\n",
    "sa2_gdf=sa2_gdf.rename(columns={'sa3_code16':'sa3_code', 'sa3_name16':'sa3_name', 'sa4_code16':'sa4_code', 'sa4_name16':'sa4_name', 'gcc_code16':'gcc_code', 'gcc_name16':'gcc_name', 'ste_code16':'ste_code', 'ste_name16':'ste_name'})\n",
    "sa2_gdf['sa2_main16'] = sa2_gdf['sa2_main16'].astype('str')\n",
    "sa2_gdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check geometries\n",
    "print(len(sa2_gdf))\n",
    "print(sa2_gdf.geometry.type.value_counts())\n",
    "no_geoms_sa2 = sum(sa2_gdf.geometry.type.isna())\n",
    "print(f'Null geometry count: {no_geoms_sa2}')\n",
    "\n",
    "# Inspection / preliminary exploration\n",
    "print(\"\\n\")\n",
    "print(sa2_gdf.ste_name.value_counts()) # Confirm federal level dataset\n",
    "print(\"\\n\")\n",
    "print(sa2_gdf.loc[sa2_gdf.ste_name==\"New South Wales\"]['gcc_name'].value_counts()) # Confirm Greater Sydney GCCSA\n",
    "\n",
    "# Process to support report writing - confirm GS vs RONSW split of provided neighbourhoods\n",
    "nbhd_join_sa2 = pd.merge(nbhd_df, sa2_gdf, left_on='area_id', right_on='sa2_main16')\n",
    "print('\\n')\n",
    "print(nbhd_join_sa2.gcc_name.value_counts()) # 312 GS and 10 RONSW\n",
    "print(\"\\nThe assignment involves devising risk scores for all 312 Greater Sydney SA2s\\nand the following 10 other NSW SA2s:\\n\")\n",
    "nbhd_join_sa2.loc[nbhd_join_sa2.gcc_name == \"Rest of NSW\"]['area_name'].to_list()\n",
    "\n",
    "# DECISION: WE RETAIN THE 18 ROWS WHERE NO SPATIAL JOINS OR FUNCTIONS CAN BE PERFORMED (NULL GEOMETRIES)\n",
    "# TO MAINTAIN FULL SA2 SHAPEFILE IN OUR DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional dataset: Telephone Exchanges NSW\n",
    "#### Geodataframe created from a retrieved JSON from ArcGIS REST Web Service provided by Geoscience Australia\n",
    "Open API - credentials are not required to run the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geoscience Australia - National Telephone Exchanges ArcGIS REST Web Service (Open API - no key required)\n",
    "# https://services.ga.gov.au/gis/rest/services/Telephone_Exchanges/MapServer/0\n",
    "# Usage permitted under Creative Commons Attribution 4.0 International Licence\n",
    "\n",
    "# This service requires a relational model style syntax for its parameters, \n",
    "# e.g. WHERE (STATE)='New South Wales'\n",
    "# These params then need to be converted to URL encoded characters for making the endpoint GET request\n",
    "\n",
    "# We query the active communications exchanges in New South Wales to form our additional dataset\n",
    "# with parameter: format = json\n",
    "\n",
    "query_param = \"%28STATE%29%3D%27New+South+Wales%27\" # (STATE)='New South Wales'\n",
    "response = requests.get(\"https://services.ga.gov.au/gis/rest/services/Telephone_Exchanges/MapServer/0/query?where=\"+\n",
    "                        query_param+\n",
    "                        \"&f=json\")\n",
    "\n",
    "assert response.status_code == 200\n",
    "\n",
    "exchanges_json = response.json()\n",
    "\n",
    "# We have inspected the full JSON result in previous requests and deploy the keys accordingly\n",
    "\n",
    "print('EPSG:\\n' + str(exchanges_json['spatialReference'])) # Confirmed GDA94\n",
    "\n",
    "# Convert results to pandas dataframe\n",
    "names_recs = []\n",
    "longs = []\n",
    "lats = []\n",
    "\n",
    "for i in range(0, len(exchanges_json['features'])):\n",
    "    item_name = exchanges_json['features'][i]['attributes']['name']\n",
    "    item_long = exchanges_json['features'][i]['geometry']['x']\n",
    "    item_lat = exchanges_json['features'][i]['geometry']['y']\n",
    "    names_recs.append(item_name)\n",
    "    longs.append(item_long)\n",
    "    lats.append(item_lat)\n",
    "\n",
    "names_series = pd.Series(names_recs)\n",
    "longs_series = pd.Series(longs)\n",
    "lats_series = pd.Series(lats)\n",
    "\n",
    "exchanges_df = pd.DataFrame({'name': names_series, 'longitude': longs_series,\n",
    "                            'latitude': lats_series})\n",
    "\n",
    "# Convert the df to geodataframe where longitude and latitude are combined into POINT geometries\n",
    "exchanges_gdf = gpd.GeoDataFrame(exchanges_df,\n",
    "                                       geometry=gpd.points_from_xy(exchanges_df.longitude, exchanges_df.latitude))\n",
    "\n",
    "exchanges_gdf = exchanges_gdf.drop(columns=['longitude', 'latitude'])\n",
    "exchanges_gdf.plot(color='white', edgecolor='red')\n",
    "\n",
    "exchanges_gdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the service, the point that appears to be in Victoria is Dareton, which is a border town (in NSW)\n",
    "exchanges_df.loc[exchanges_df['latitude'] < -38]\n",
    "\n",
    "# The coordinates for this point are likely an error but this town would not be in the analysis anyway\n",
    "# As nature of the error is unresolved, the row is retained in the DB design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting with Database and Creation of Tables\n",
    "#### *Running connection function with credentials*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ATTN MARKER - DO *NOT* RUN** cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------ATTN MARKER - DO *NOT* RUN---------------\n",
    "# Alternative Function for accessing Postgres DB (SOURCE: DATA2001 Lab materials) - Matthew\n",
    "# JUST FOR MATTHEW\n",
    "\n",
    "def pgconnect_using_credfile(credential_filepath):\n",
    "    try:\n",
    "        args = {\n",
    "            'sslmode':'disable',\n",
    "            'gssencmode':'disable'\n",
    "        }\n",
    "        with open(credential_filepath) as f:\n",
    "            db_conn_dict = json.load(f)\n",
    "        connstring = 'postgresql+psycopg2://'+db_conn_dict['user']+':'+db_conn_dict['password']+'@'+db_conn_dict['host']+'/'+db_conn_dict['database']\n",
    "        db = create_engine(connstring, echo=False, connect_args=args)\n",
    "        conn = db.connect()\n",
    "        print('connected')\n",
    "    except Exception as e:\n",
    "        print(\"unable to connect to the database\")\n",
    "        print(e)\n",
    "        return None\n",
    "    return db,conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to University server student Postgres DB. \n",
    "# This function will not work if not on campus or if not connected to VPN.\n",
    "\n",
    "credfilepath = './data2x01_db.json' # Internal note: not tracked on Git, must be locally available. \n",
    "# Eugene's credentials JSON to be uploaded in submission\n",
    "\n",
    "db, conn = pgconnect_using_credfile(credfilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test connection and pgquery function with PostGIS (Postgres geospatial plugin) check*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking we have PostGIS working on our connection (SOURCE: DATA2001 Lab materials)\n",
    "\n",
    "postgis_check = '''\n",
    "SELECT PostGIS_Version();\n",
    "'''\n",
    "\n",
    "retval,retdf = pgquery(conn,postgis_check)\n",
    "retdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Database Tables (from dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing tables in Postgres DB public schema\n",
    "for table in db.table_names():\n",
    "    print(f'{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering set relationships between datasets for key designations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing set relationships between tables\n",
    "test_df = stat_areas_df.copy() # 431 unique area_ids exist in statisticalareas\n",
    "subset = set(nbhd_df.area_id.unique()) # 322 unique area_ids exist in neighbourhoods\n",
    "test_df['exists'] = stat_areas_df.area_id.map(lambda x : True if x in subset else False)\n",
    "print(test_df['exists'].value_counts())\n",
    "\n",
    "# Statisticalareas contains a set of area_ids of which the set of neighbourhoods area_id values is a subset \n",
    "# (It is a PK in its own table and an FK in relation to statisticalareas)\n",
    "\n",
    "# Note that there exists one more SA2 length ID in statisticalareas and that these 323 SA2s exist in the shapefile\n",
    "sa2s_in_statareas = stat_areas_df.loc[stat_areas_df['area_id'].str.len() == 9]\n",
    "sa2s_in_statareas.loc[~sa2s_in_statareas.area_id.isin(nbhd_df['area_id'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 323 SA2s in statisticalareas are a subset of the national SA2 shapefile attribute sa2_main16\n",
    "# A foreign key relationship from area_id in both neighbourhoods and statisticalareas \n",
    "# to sa2_main16 would be appropriate *IF* sa2_main16 were a primary key, however\n",
    "# g_id has been designated PK; it is preferable that the 'area_id' meaning\n",
    "# is preserved as definitive in the context where it can include other levels of statistical area (statisticalareas)\n",
    "\n",
    "sa2s_in_statareas = stat_areas_df.loc[stat_areas_df['area_id'].str.len() == 9]\n",
    "print(len(sa2s_in_statareas))\n",
    "\n",
    "test_df2 = sa2_gdf.copy()\n",
    "subset2 = set(sa2s_in_statareas.area_id.unique())\n",
    "test_df2['exists'] = sa2_gdf.sa2_main16.map(lambda x : True if x in subset2 else False)\n",
    "test_df2['exists'].value_counts()\n",
    "\n",
    "# Finally, from a cleaning step which masked businessstats with neighbourhoods area_ids, \n",
    "# we know that businessstats will hold the same key relationships with statisticalareas as neighbourhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informing varchar length limits\n",
    "The max string length will be used to the character limits for attributes where we reasonably assume systematic naming in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max string lengths occurring in stat_areas_df columns:\\n')\n",
    "print(pd.Series({c: stat_areas_df[c].map(lambda x: len(str(x))).max() for c in stat_areas_df}).sort_values(ascending=False))\n",
    "\n",
    "print('\\nMax string lengths occurring in nbhd_df columns:\\n')\n",
    "print(pd.Series({c: nbhd_df[c].map(lambda x: len(str(x))).max() for c in nbhd_df}).sort_values(ascending=False))\n",
    "\n",
    "print('\\nMax string lengths occurring in busi_stat_df columns:\\n')\n",
    "print(pd.Series({c: busi_stat_df[c].map(lambda x: len(str(x))).max() for c in busi_stat_df}).sort_values(ascending=False))\n",
    "\n",
    "# No attributes in RFS are appropriate for an assumed max varchar length for future entries\n",
    "\n",
    "print('\\nMax string lengths occurring in sa2_gdf columns:\\n')\n",
    "print(pd.Series({c: sa2_gdf[c].map(lambda x: len(str(x))).max() for c in sa2_gdf}).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF RUNNING AGAIN, THE DROP QUERY REQUIRES CASCADE BECAUSE OF FK RELATIONSHIPS \n",
    "\n",
    "conn.execute(\"DROP TABLE IF EXISTS statisticalareas CASCADE\")\n",
    "#conn.execute(\"DROP TABLE IF EXISTS statisticalareas\")\n",
    "\n",
    "stat_areas_create = '''CREATE TABLE statisticalareas (\n",
    "                     area_id VARCHAR(9) NOT NULL,\n",
    "                     area_name VARCHAR NOT NULL,\n",
    "                     parent_area_id VARCHAR(5) NOT NULL,\n",
    "                     CONSTRAINT statisticalareas_pkey PRIMARY KEY (area_id)\n",
    "                     )'''\n",
    "\n",
    "conn.execute(stat_areas_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert df data\n",
    "stat_areas_df.to_sql('statisticalareas', con = conn, if_exists = 'append', index=False)\n",
    "print('Data inserted into Table')\n",
    "\n",
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM statisticalareas\n",
    "LIMIT 1;\"\"\")\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS neighbourhoods\")\n",
    "\n",
    "neighbourhoods_create = '''CREATE TABLE neighbourhoods (\n",
    "                     area_id CHAR(9) NOT NULL,\n",
    "                     area_name VARCHAR NOT NULL,\n",
    "                     land_area FLOAT NOT NULL,\n",
    "                     population NUMERIC,\n",
    "                     dwellings NUMERIC NOT NULL,\n",
    "                     businesses NUMERIC,\n",
    "                     median_income NUMERIC,\n",
    "                     avg_monthly_rent NUMERIC,\n",
    "                     CONSTRAINT neighbourhoods_pkey PRIMARY KEY (area_id),\n",
    "                     CONSTRAINT neighbourhoods_fkey1 FOREIGN KEY(area_id) REFERENCES statisticalareas(area_id)\n",
    "                     )'''\n",
    "\n",
    "# ALLOW NULL for 'population' 'businesses' avg_monthly_rent' 'median_income'\n",
    "\n",
    "conn.execute(neighbourhoods_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert df data\n",
    "nbhd_df.to_sql('neighbourhoods', con = conn, if_exists = 'append', index=False)\n",
    "print('Data inserted into Table')\n",
    "\n",
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM neighbourhoods\n",
    "LIMIT 1;\"\"\")\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS businessstats\")\n",
    "\n",
    "business_create = '''CREATE TABLE businessstats (\n",
    "                     area_id CHAR(9) NOT NULL,\n",
    "                     number_of_businesses NUMERIC NOT NULL,\n",
    "                     accommodation_and_food NUMERIC NOT NULL,\n",
    "                     retail_trade NUMERIC NOT NULL,\n",
    "                     agriculture_forestry_and_fishing NUMERIC NOT NULL,\n",
    "                     health_care_and_social_assistance NUMERIC NOT NULL,\n",
    "                     public_administration_and_safety NUMERIC NOT NULL,\n",
    "                     transport_postal_and_warehousing NUMERIC NOT NULL,\n",
    "                     CONSTRAINT businessstats_pkey PRIMARY KEY (area_id),\n",
    "                     CONSTRAINT businessstats_fkey1 FOREIGN KEY(area_id) REFERENCES statisticalareas(area_id)\n",
    "                     )'''\n",
    "\n",
    "conn.execute(business_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert df data\n",
    "busi_stat_df.to_sql('businessstats', con = conn, if_exists = 'append', index=False)\n",
    "print('Data inserted into Table')\n",
    "\n",
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM businessstats\n",
    "LIMIT 1;\"\"\")\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Database Tables (from geodataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFS NSW Bushfire Prone Land - shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srid = 4283\n",
    "rfs_gdf['geom'] = rfs_gdf['geometry'].apply(lambda x: create_wkt_point_element(geom=x, srid=srid))\n",
    "rfs_gdf = rfs_gdf.drop(columns=\"geometry\")\n",
    "rfs_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS rfsnsw_bfpl\")\n",
    "\n",
    "rfs_bushfire_create = '''CREATE TABLE rfsnsw_bfpl (\n",
    "                     gid INTEGER PRIMARY KEY,\n",
    "                     category CHAR(1),\n",
    "                     shape_leng FLOAT,\n",
    "                     shape_area FLOAT,\n",
    "                     geom GEOMETRY(POINT, 4283)\n",
    "                     )'''\n",
    "\n",
    "conn.execute(rfs_bushfire_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert gdf data\n",
    "srid = 4283\n",
    "rfs_gdf.to_sql('rfsnsw_bfpl', conn, if_exists='append', index=False, \n",
    "                         dtype={'geom': Geometry('POINT', srid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM rfsnsw_bfpl\n",
    "LIMIT 1;\"\"\")\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ABS Statistical Area 2 (2016) - shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL NONE GEOMETRIES WITH EMPTY GEOMETRY OBJECTS SO THAT THEY CAN BE PERSISTED\n",
    "# Note: a warning will be produced regarding meaning of 'isna()' in geopandas\n",
    "\n",
    "# See: https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoSeries.fillna.html\n",
    "\n",
    "sa2_gdf['geometry'] = sa2_gdf['geometry'].fillna()\n",
    "print(len(sa2_gdf.loc[sa2_gdf.geometry.isna()]))\n",
    "\n",
    "srid = 4283\n",
    "\n",
    "# WKT CONVERSION\n",
    "sa2_gdf['geom'] = sa2_gdf['geometry'].apply(lambda x: create_wkt_element(geom=x, srid=srid))\n",
    "sa2_gdf = sa2_gdf.drop(columns=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS sa2_2016_aust\")\n",
    "\n",
    "sa2_shape_create = '''CREATE TABLE sa2_2016_aust (\n",
    "                     g_id INTEGER PRIMARY KEY,\n",
    "                     sa2_main16 CHAR(9),\n",
    "                     sa2_5dig16 CHAR(5),\n",
    "                     sa2_name16 VARCHAR,\n",
    "                     sa3_code   CHAR(5),\n",
    "                     sa3_name   VARCHAR,\n",
    "                     sa4_code   CHAR(3),\n",
    "                     sa4_name   VARCHAR,\n",
    "                     gcc_code   CHAR(5),\n",
    "                     gcc_name   VARCHAR,\n",
    "                     ste_code   CHAR(1),\n",
    "                     ste_name   VARCHAR,\n",
    "                     areasqkm16 FLOAT,\n",
    "                     geom GEOMETRY(MULTIPOLYGON, 4283)\n",
    "                     )'''\n",
    "\n",
    "conn.execute(sa2_shape_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert gdf data\n",
    "srid = 4283\n",
    "sa2_gdf.to_sql('sa2_2016_aust', conn, if_exists='append', index=False, \n",
    "                         dtype={'geom': Geometry('MULTIPOLYGON', srid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM sa2_2016_aust WHERE g_id = 2308\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creation of Database Table - Additional Dataset (Telephone exchanges geodataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use create_wkt_point_element function to create point geom column\n",
    "\n",
    "srid = 4283\n",
    "exchanges_gdf['geom'] = exchanges_gdf['geometry'].apply(lambda x: create_wkt_point_element(geom=x, srid=srid))\n",
    "exchanges_gdf = exchanges_gdf.drop(columns=\"geometry\")\n",
    "exchanges_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS exchanges\")\n",
    "\n",
    "exchanges_create = '''CREATE TABLE exchanges (\n",
    "                     name VARCHAR NOT NULL,\n",
    "                     geom GEOMETRY(POINT, 4283),\n",
    "                     CONSTRAINT exchanges_pkey PRIMARY KEY (name) \n",
    "                     )'''\n",
    "\n",
    "conn.execute(exchanges_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert gdf data\n",
    "srid = 4283\n",
    "exchanges_gdf.to_sql('exchanges', conn, if_exists='append', index=False,\n",
    "                     dtype={'geom': Geometry('POINT', srid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\" SELECT * FROM exchanges WHERE name = 'Glebe' \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fire Risk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of helpful indexes\n",
    "\n",
    "The Bushfire Prone Land dataset is very large but we cannot refine it beforehand - during computation of the measure we need to make spatial comparisons for all of its entries using PostGIS on the Postgres server. The computation is intensive and slow so it is helpful to employ a spatial index by creating a GIST index on the rfsnsw_bfpl geom column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP INDEX IF EXISTS bfpl_geom_idx\")\n",
    "\n",
    "bf_index_create = '''CREATE INDEX bfpl_geom_idx\n",
    "                          ON rfsnsw_bfpl\n",
    "                      USING GIST (geom);'''\n",
    "\n",
    "conn.execute(bf_index_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are computing membership and relationships with the SA2 shapefiles, spatial queries involving sa2_2016_aust will benefit from this kind of index as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"DROP INDEX IF EXISTS sa2_geom_idx\")\n",
    "\n",
    "sa2_index_create = '''CREATE INDEX sa2_geom_idx\n",
    "                          ON sa2_2016_aust\n",
    "                      USING GIST (geom);'''\n",
    "\n",
    "conn.execute(sa2_index_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Exchanges spatial data is much smaller we deemed that index creation would not have much performance benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computed Measures\n",
    "\n",
    "#### Methodology note: area used in calculation of measures\n",
    "For the following computed densities we use the sa2_2016_aust areasqkm16 column as that most accurately corresponds to area calculated based on geometry. The following query demonstrates our comparison process. \n",
    "\n",
    "There are data quality issues with the land_area column in neighbourhoods - note that some reflect a conversion from the sa2_2016_aust shapefile data to a smaller unit (e.g. Gosford-Springfield 16.9124 to 1691.2000) while others are the same (e.g. Goulburn Region at 9035.1221 in both). This is what prompted the manual check using ST_Area which affirmed the reliability of the sa2_2016_aust column. The difference in area in the pre-computed areasqkm16 vs the PostGIS result was not deemed significant enough to compute and store (or compute per query) area sizes, so areasqkm16 is used in each measure calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT n.area_name, n.land_area, sa.areasqkm16, ST_Area(sa.geom::geography)/1000000 AS total_area_km2 \n",
    "FROM neighbourhoods AS n\n",
    "INNER JOIN sa2_2016_aust AS sa\n",
    "ON n.area_id=sa.sa2_main16\n",
    "LIMIT 10;\n",
    "'''\n",
    "response, checking_df = pgquery(conn, query)\n",
    "checking_df['difference_postgis_orig'] = checking_df['total_area_km2'] - checking_df['areasqkm16']\n",
    "checking_df['difference_orig_postgis'] = checking_df['areasqkm16'] - checking_df['total_area_km2']\n",
    "print(max(checking_df['difference_postgis_orig']))\n",
    "print(max(checking_df['difference_orig_postgis']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Population Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As defined in methodology earlier, NaN value for population is interpreted as 0\n",
    "\n",
    "query = '''\n",
    "SELECT n.area_name, n.area_id, n.population, sa.areasqkm16, n.population/sa.areasqkm16 as population_density\n",
    "FROM neighbourhoods AS n\n",
    "INNER JOIN sa2_2016_aust AS sa\n",
    "ON n.area_id=sa.sa2_main16\n",
    "ORDER BY n.population/sa.areasqkm16 DESC;\n",
    "'''\n",
    "response, pop_df = pgquery(conn, query)\n",
    "\n",
    "pop_df['population'] = pop_df['population'].fillna(0)\n",
    "pop_df['population_density'] = pop_df['population_density'].fillna(0)\n",
    "\n",
    "pop_df = add_z_score_column(pop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dwelling Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dwelling and business density measures are to be summed before computing z-score as they will represent a single variable in the risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT n.area_name, n.area_id, n.dwellings, sa.areasqkm16, n.dwellings/sa.areasqkm16 as dwelling_density\n",
    "FROM neighbourhoods AS n\n",
    "INNER JOIN sa2_2016_aust AS sa\n",
    "ON n.area_id=sa.sa2_main16\n",
    "ORDER BY n.dwellings/sa.areasqkm16 DESC;\n",
    "'''\n",
    "\n",
    "response, dwell_df = pgquery(conn, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Business Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT n.area_name, n.area_id, sa.areasqkm16, busi.number_of_businesses, \n",
    "busi.number_of_businesses/sa.areasqkm16 as business_density\n",
    "FROM neighbourhoods AS n\n",
    "INNER JOIN sa2_2016_aust AS sa\n",
    "ON n.area_id=sa.sa2_main16\n",
    "INNER JOIN businessstats AS busi\n",
    "ON n.area_id=busi.area_id\n",
    "ORDER BY busi.number_of_businesses/sa.areasqkm16 DESC;\n",
    "'''\n",
    "response, bus_df = pgquery(conn, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining measures 2 (dwelling density) and 3 (business density) for single z-score (final model includes single variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_df_m = bus_df.drop(columns=['area_name','areasqkm16','number_of_businesses'])\n",
    "dwellbusi_df = pd.merge(dwell_df,bus_df_m, left_on='area_id', right_on='area_id', how='inner')\n",
    "dwellbusi_df = dwellbusi_df.drop(columns=['dwellings','areasqkm16'])\n",
    "dwellbusi_df['dwell_and_bus_density'] = dwellbusi_df['dwelling_density'] + dwellbusi_df['business_density']\n",
    "dwellbusi_df = dwellbusi_df.drop(columns=['dwelling_density','business_density'])\n",
    "dwellbusi_df = add_z_score_column(dwellbusi_df)\n",
    "dwellbusi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. BFPL Density\n",
    "\n",
    "For this step a spatial join is performed in the database and then further computations are performed in pandas.\n",
    "\n",
    "Step 1: SQL query that produces an output table showing matching shapefile containment relationship for every point, filtered for only the neighbourhood of interest SA2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNINGS PRODUCED WILL PREVENT PRINT OF SQL RESULT IN THE NOTEBOOK, HOWEVER DF WILL BE CREATED\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT * FROM\n",
    "(\n",
    "SELECT bf.gid, bf.category, bf.shape_leng, bf.shape_area, sa.sa2_main16, sa.sa2_name16\n",
    "FROM rfsnsw_bfpl AS bf\n",
    "    INNER JOIN sa2_2016_aust AS sa ON ST_Contains(sa.geom, bf.geom)\n",
    ") AS nsw_all_bf\n",
    "WHERE nsw_all_bf.sa2_main16 IN\n",
    "(\n",
    "SELECT sa.sa2_main16\n",
    "FROM neighbourhoods AS n\n",
    "    INNER JOIN sa2_2016_aust AS sa\n",
    "    ON n.area_id=sa.sa2_main16 \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "response, bfpl_step1_df = pgquery(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfpl_step1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: \n",
    "- Computing weightings for area based on vegetation category \n",
    "- and then aggregating total area per SA2 using pandas groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weighting different vegetation categories\n",
    "# Based on interpretation of language regarding relative risk levels and difference in required buffer distances \n",
    "# outlined in RFS Guideline for Councils to Bushfire Prone Area Land Mapping p. 11\n",
    "# Note the category number coding does not represent risk ranking (while 1 is highest, 3 is medium, 2 is least)\n",
    "# Inline function for this task based on post here: \n",
    "# https://stackoverflow.com/questions/41962022/apply-function-to-dataframe-column-element-based-on-value-in-other-column-for-sa\n",
    "\n",
    "def category_weighter(number,condition):\n",
    "    multiplier = {'1': 3, '2': 1, '3': 1.5}\n",
    "    return number * multiplier[condition]\n",
    "\n",
    "bfpl_step1_df['weighted_bfp_area'] = bfpl_step1_df.apply(lambda x: category_weighter(x['shape_area'],\n",
    "                                                                                     x['category']), axis=1)\n",
    "\n",
    "bfpl_step2_df = bfpl_step1_df.groupby(['sa2_main16', 'sa2_name16']).agg('weighted_bfp_area').sum().reset_index()\n",
    "bfpl_step2_df.sort_values(by='weighted_bfp_area',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "- Using a join in pandas to reintroduce the neighbourhoods that contain 0 BFPL\n",
    "- Computing final density (weighted_bfp_area total/areasqkm16), filling NaNs with 0s\n",
    "- and computing z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfpl_step3_df = pop_df.copy()\n",
    "bfpl_step3_df = bfpl_step3_df.drop(columns=['population','population_density','z_score'])\n",
    "bfpl_step3_df = pd.merge(bfpl_step3_df, bfpl_step2_df, left_on='area_id', right_on='sa2_main16', how='left')\n",
    "bfpl_step3_df = bfpl_step3_df.drop(columns=['sa2_main16','sa2_name16'])\n",
    "bfpl_step3_df['bfpl_density'] = bfpl_step3_df['weighted_bfp_area']/bfpl_step3_df['areasqkm16']\n",
    "\n",
    "bfpl_step3_df['weighted_bfp_area'] = bfpl_step3_df['weighted_bfp_area'].fillna(0)\n",
    "bfpl_step3_df['bfpl_density'] = bfpl_step3_df['bfpl_density'].fillna(0)\n",
    "bfpl_step3_df = add_z_score_column(bfpl_step3_df)\n",
    "\n",
    "bfpl_density = bfpl_step3_df.copy()\n",
    "bfpl_density.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Assistive Service Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT n.area_name, n.area_id, sa.areasqkm16, busi.health_care_and_social_assistance,\n",
    "busi.health_care_and_social_assistance/sa.areasqkm16 as assistive_service_density\n",
    "FROM neighbourhoods AS n\n",
    "INNER JOIN sa2_2016_aust AS sa\n",
    "ON n.area_id=sa.sa2_main16\n",
    "INNER JOIN businessstats AS busi\n",
    "ON n.area_id=busi.area_id\n",
    "ORDER BY busi.health_care_and_social_assistance/sa.areasqkm16 DESC;\n",
    "'''\n",
    "\n",
    "response, ass_serv_df = pgquery(conn, query)\n",
    "ass_serv_df = add_z_score_column(ass_serv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Exchange Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume truth of Geoscience Australia communications exchanges dataset \n",
    "# i.e. NAN results from spatial subquery join == 0 exchanges in area and therefore 0 density\n",
    "# CASE clause handles the columns which we ultimately pass to our df (so no NaNs dealt with at notebook end)\n",
    "\n",
    "query='''\n",
    "SELECT n.area_name, n.area_id, nsw_result.count AS exchanges_nans, sa.areasqkm16,\n",
    "nsw_result.count/sa.areasqkm16 AS exchange_density_nans,\n",
    "CASE\n",
    "WHEN nsw_result.count IS NULL THEN 0\n",
    "    ELSE nsw_result.count\n",
    "END AS exchanges,\n",
    "CASE\n",
    "    WHEN nsw_result.count/sa.areasqkm16 IS NULL THEN 0\n",
    "    ELSE nsw_result.count/sa.areasqkm16\n",
    "END AS exchange_density\n",
    "FROM neighbourhoods AS n\n",
    "    INNER JOIN sa2_2016_aust AS sa\n",
    "    ON n.area_id=sa.sa2_main16 \n",
    "    LEFT JOIN\n",
    "        (\n",
    "            SELECT sa.sa2_main16, sa.sa2_name16, COUNT(e.geom)\n",
    "            FROM sa2_2016_aust AS sa\n",
    "                INNER JOIN exchanges AS e ON ST_Contains(sa.geom, e.geom)\n",
    "            GROUP BY sa.sa2_main16, sa.sa2_name16\n",
    "        ) AS nsw_result\n",
    "ON n.area_id = nsw_result.sa2_main16\n",
    "ORDER BY nsw_result.count/sa.areasqkm16;\n",
    "\n",
    "'''\n",
    "\n",
    "response, exchange_df = pgquery(conn, query)\n",
    "exchange_df = exchange_df.drop(columns=['exchanges_nans','exchange_density_nans'])\n",
    "exchange_df = add_z_score_column(exchange_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Risk Score\n",
    "\n",
    "Provided model:\n",
    "\n",
    "$$fire_risk = S(z(population_density)+z(dwelling_&_business_density)+z(bfpl_density)âˆ’z(assistive_service_density))$$\n",
    "\n",
    "Refined model:\n",
    "\n",
    "$$fire_risk = S(0.1*z(population_density)+0.1*z(dwelling_&_business_density)+3*z(bfpl_density)âˆ’0.5*z(assistive_service_density-0.5*z(exchange_density))$$\n",
    "\n",
    "While significantly reweighting the model towards bushfire prone land density, we justify these weightings because the other variables, even with decreased weighting, still contribute to construct relative risk based on areas that are:\n",
    "- populated\n",
    "- under-serviced\n",
    "- contain bushfire prone land\n",
    "- contain relatively greater amounts of the higher risk vegetation categories (based on weightings done in preprocessing work)\n",
    "\n",
    "Without refinement, the model is too sensitive to populated urban areas which have zero BFPL and should not receive high risk scores. BFPL is the only natural environmental variable in the model and the model determines a risk assesment of an environmental phenomenon so this has informed the significant weighting allocated.\n",
    "\n",
    "Though the nominated coefficients would appear to potentially overwhelm the model with this variable, the computed fire risk score rankings are different to the ranked order of bushfire prone land density alone, in particular there is different relative positioning outside the top quintile of BFPL density neighbourhoods. For example, the Manly - Fairlight area is ranked 38th in BFPL density, while its fire risk score is ranked 47th, which demonstrates the model recognizes the mitigation of the BFPL risk by the two negative determinants effective in that area. While there is a heavy bias imparted by BFPL density it does not dominate the model at the expense of the informative value of the other determinants.\n",
    "\n",
    "#### Creation of dataframe with all measures\n",
    "This is the first step for the creation of the computed risk score table. A series of pandas joins integrates the computed measures into a single dataframe while ensuring correct matching with area_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_risk_df = pop_df.copy()\n",
    "fire_risk_df = fire_risk_df.drop(columns=['population','areasqkm16'])\n",
    "fire_risk_df = fire_risk_df.rename(columns={\"z_score\": \"pop_z_score\"})\n",
    "\n",
    "fire_risk_df = pd.merge(fire_risk_df, dwellbusi_df, left_on='area_id', right_on='area_id', how='inner')\n",
    "fire_risk_df = fire_risk_df.drop(columns=['area_name_y'])\n",
    "fire_risk_df = fire_risk_df.rename(columns={\"z_score\": \"dwell_and_bus_z_score\"})\n",
    "\n",
    "fire_risk_df = pd.merge(fire_risk_df, bfpl_density, left_on='area_id', right_on='area_id', how='inner')\n",
    "fire_risk_df = fire_risk_df.drop(columns=['area_name','areasqkm16','weighted_bfp_area'])\n",
    "fire_risk_df = fire_risk_df.rename(columns={\"z_score\": \"bf_z_score\"})\n",
    "\n",
    "fire_risk_df = pd.merge(fire_risk_df, ass_serv_df, left_on='area_id', right_on='area_id', how='inner')\n",
    "fire_risk_df = fire_risk_df.drop(columns=['area_name','areasqkm16','health_care_and_social_assistance'])\n",
    "fire_risk_df = fire_risk_df.rename(columns={\"z_score\": \"asst_z_score\"})\n",
    "\n",
    "fire_risk_df = pd.merge(fire_risk_df, exchange_df, left_on='area_id', right_on='area_id', how='inner')\n",
    "fire_risk_df = fire_risk_df.drop(columns=['area_name','areasqkm16','exchanges'])\n",
    "fire_risk_df = fire_risk_df.rename(columns={\"z_score\": \"exch_z_score\"})\n",
    "\n",
    "fire_risk_df = fire_risk_df.rename(columns={'area_name_x': 'area_name'})\n",
    "\n",
    "fire_risk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of fire risk scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominated coefficients to address confounding variable of population and the relative importance of BFPL\n",
    "# 0.1(population_density)\n",
    "# 0.1(dwelling_&_business_density)\n",
    "# 3(bfpl_density)\n",
    "# 0.5(assistive_service_density)\n",
    "# 0.5(exchange_density)\n",
    "\n",
    "fire_risk_df['untransformed'] = 0.1*(fire_risk_df['pop_z_score']) + 0.1*(fire_risk_df['dwell_and_bus_z_score']) + 3*(fire_risk_df['bf_z_score']) - 0.5*(fire_risk_df['asst_z_score']) - 0.5*(fire_risk_df['exch_z_score'])\n",
    "fire_risk_df['fire_risk_score'] = fire_risk_df['untransformed'].apply(lambda x: sigmoid(x))\n",
    "fire_risk_df.sort_values(by='fire_risk_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration of the computed measures and risk scores into the Postgres database\n",
    "\n",
    "Note that the table for computed fire risk includes the computed measures per neighbourhood (direct, not z-score) and the final fire risk score per neighbourhood (which is the result of computations done in Python code, which involved use of z-scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataframe to: measures and risk score and normalised design (no 'area_name')\n",
    "fire_risk_df = fire_risk_df.drop(columns=['area_name','pop_z_score','dwell_and_bus_z_score','bf_z_score','asst_z_score','exch_z_score','untransformed'])\n",
    "\n",
    "# Table creation query\n",
    "conn.execute(\"DROP TABLE IF EXISTS fire_risk\")\n",
    "\n",
    "score_create = '''CREATE TABLE fire_risk (\n",
    "                     area_id CHAR(9) NOT NULL,\n",
    "                     population_density FLOAT NOT NULL,\n",
    "                     dwell_and_bus_density FLOAT NOT NULL,\n",
    "                     bfpl_density FLOAT NOT NULL,\n",
    "                     assistive_service_density FLOAT NOT NULL,\n",
    "                     exchange_density FLOAT NOT NULL,\n",
    "                     fire_risk_score FLOAT NOT NULL,\n",
    "                     CONSTRAINT fire_risk_pkey PRIMARY KEY (area_id),\n",
    "                     CONSTRAINT fire_risk_fkey1 FOREIGN KEY(area_id) REFERENCES statisticalareas(area_id)\n",
    "                     )'''\n",
    "\n",
    "conn.execute(score_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_risk_df.to_sql('fire_risk', con = conn, if_exists = 'append', index=False)\n",
    "print('Data inserted into Table')\n",
    "\n",
    "# Check table\n",
    "a_response, a_df = pgquery(conn, \"\"\"SELECT * FROM fire_risk\n",
    "LIMIT 1;\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Choropleth Map of Risk Scores\n",
    "\n",
    "The following code is to generate a choropleth map of the fire risk scores across the New South Wales neighbourhoods studied. This is done directly from a query to the database. As a result, the geometry needs to be converted back from the WKTElement used by PostGIS to the geometry used by geopandas. (Note that for the map figure there is an implicit dependency on a library called descartes but this is available in the University server).\n",
    "\n",
    "If query output is produced when running the below cell, please scroll to the bottom of the cell output in order to view the choropleth map (also included in Report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNINGS PRODUCED WILL PREVENT PRINT OF SQL RESULT IN THE NOTEBOOK, HOWEVER DF WILL BE CREATED\n",
    "\n",
    "response, risk_score_df = pgquery(conn, '''\n",
    "SELECT area_id, fire_risk_score, ST_AsEWKT(geom::geometry)\n",
    "FROM fire_risk AS fr INNER JOIN sa2_2016_aust AS sa ON(fr.area_id = sa.sa2_main16)\n",
    "''') # First extract geometry as a WKTElement in string from SQL.\n",
    "\n",
    "print(type(risk_score_df.st_asewkt[0])) \n",
    "\n",
    "def str_to_geom(x): # Removes unnecessary SRID and uses shapely wkt module to convert string to shapely geometry object\n",
    "    x=x.replace('SRID=4283;', '')\n",
    "    print(x)\n",
    "    x=wkt.loads(x)\n",
    "    return x\n",
    "\n",
    "risk_score_df['geom'] = risk_score_df['st_asewkt'].map(lambda x: str_to_geom(x))\n",
    "risk_score_df=risk_score_df.drop(columns=['st_asewkt'])\n",
    "\n",
    "print(type(risk_score_df.geom[0]))\n",
    "\n",
    "risk_score_df = GeoDataFrame(risk_score_df, crs='EPSG:4283', geometry=risk_score_df['geom'])\n",
    "risk_score_df = risk_score_df.drop(columns=['geom'])\n",
    "\n",
    "# Choropleth code\n",
    "# Adapted from https://medium.com/@m_vemuri/create-a-geographic-heat-map-of-the-city-of-toronto-in-python-cd2ae0f8be55\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(40, 20))\n",
    "ax.set_title('Fire Risk Score across Greater Sydney Neighbourhoods (New South Wales)', fontdict={'fontsize':'20'})\n",
    "ax.axis('off')\n",
    "color = 'Oranges'\n",
    "vmin, vmax = 0, 1\n",
    "sm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm._A=[]\n",
    "cbar = fig.colorbar(sm)\n",
    "risk_score_df.plot('fire_risk_score', cmap=color, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression & Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis function producing these results is performed with a live query to the relevant database attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "For each combination of regressor i.e. median income and average monthly rent, and regressand i.e. fire risk score.\n",
    "\n",
    "* Residuals plot is first generated to check for homescedasticity assumptions so that a linear model can be applied. \n",
    "* Ordinary-least-squares method of linear regression is applied, visualised in a scatter plot along with the predicted linear model. \n",
    "* A linear model summary which most importantly includes:\n",
    "    * The r2 Score which gives us an indication of how much variance in the regressand can be explained by the regressor. \n",
    "    * The p-value which gives us an indication of the significance of the parameter. The null hypothesis is that the coefficient == 0. Therefore with a higher p-value we would retain this hypothesis and it would be evidence for no relationship. Correspondingly, a p-value below the alpha threshold would mean rejection of this hypothesis and would be statistically significant evidence of a non-zero value for the coefficient, suggesting a relationship between independent and dependent variables.\n",
    "    * Pearson correlation coefficient which gives us an indication of the strength and direction of the linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fire Risk Score regressed on Median Income\n",
    "Based on the following analysis it is unlikely that median income can effectively explain fire risk score.\n",
    "\n",
    "* The residuals plot shows that the data meets the assumption of homoscedasticity and the residuals are fairly normally distributed.\n",
    "* The r2 score is very low indicating that the variation in fire risk score isn't well explained by median income.\n",
    "* The p value is > 0.05, supporting retention of the null hypothesis (coefficient == 0)\n",
    "* The correlation value indicates a very weak negative linearity.\n",
    "\n",
    "*(Note: due to the pgquery function output that is also produced, please scroll to the bottom of the output to see the charts inside the notebook (they are also included in Report))*\n",
    "\n",
    "*(Note: in the University server environment we need to import statsmodels in the manner below each time the function is run - this was not required in our original notebook but this adaptation should allow the cell to run without error in the marking environment)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "LinearReg('fire_risk_score', 'median_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fire Risk Score regressed on Average Monthly Rent\n",
    "\n",
    "Based on the following analysis it is likely that average monthly rent could be one factor that explains fire risk score.\n",
    "\n",
    "* The residuals plot shows that the data meets the assumption of homoscedasticity and the residuals are fairly normally distributed.\n",
    "* The r2 score is relatively low indicating that the variation in fire risk score is only partially explained by median income.\n",
    "* The p value is < 0.05 (in fact too small for the OLS results summary to display beyond 0.000), allowing rejection of the null hypothesis that the coefficient == 0 and providing support for a non-zero coefficient for the variable.\n",
    "* The correlation value indicates some negative linearity.\n",
    "\n",
    "*(Note: due to the pgquery function output that is also produced, please scroll to the bottom of the output to see the charts inside the notebook (they are also included in Report))*\n",
    "\n",
    "*(Note: in the University server environment we need to import statsmodels in the manner below each time the function is run - this was not required in our original notebook but this adaptation should allow the cell to run without error in the marking environment)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "LinearReg('fire_risk_score', 'avg_monthly_rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "#     REMEMBER: DISCONNECT FROM DB!\n",
    "# ------------------------------------\n",
    "\n",
    "conn.close()\n",
    "db.dispose()\n",
    "print(\"disconnected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
